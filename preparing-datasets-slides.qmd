---
title: "Preparing Datasets for Deposit"
author: "Michael Lenard, Research Data Management Librarian"
date: "February 28, 2024"
format: 
  revealjs:
    theme: default
editor: visual
---

## Before we begin

-   download the .zip file from the GitHub repository
-   put it somewhere easy to find
-   unzip it

::: {.aside}
**Disclaimer**: Sharing human subjects research data involves ethical and procedural complications we won't have time to get into today
:::

## Motivation

Why should we take care to prepare our datasets before deposit?

-   Saves time and effort later
-   Well cared for datasets reflect well conducted studies
-   Facilitates reuse of your dataset (can't be reused if no one can find it or make sense of it)
-   Some repositories expect some level of curation has been done by you

## Research Data Lifecycle

One example from UVA Library:

![](images/data-lifecycle.png)

::: {.aside}
There are many other examples, you may find them more or less useful
:::

## Topic Overview

1.  File organization/directory structure
2.  Data cleaning
3.  Documentation and metadata
4.  Selecting a repository

## File Organization

Some general principles for file organization:

1. Plan it early
2. Write it down
3. Make it logical
4. Follow it consistently
5. Avoid encoding information in the directory structure

::: {.notes}
    Plan it early: Don't wait until partway through a project to devise an organization plan. It will be easier to follow a plan if you do it from the beginning, before it involves moving or renaming anything.
    
    Write it down: You will want to document your file organization scheme for several reasons, including making it easier for students or collaborators to follow, ensuring you understand the organization if you come back to a project after a long pause, and for use in your final documentation that you submit to a repository at the end of the project.
    
    Make it logical: There won't ever be only one logical organization for any project, but some choices are better than others. Beyond organizing your files and folders by project, you might choose to organize projects by
        Time period
        Researcher/project staff
        File type or function
        Location
        Research activity or experiment
        Specimen
        Or some other way your data and files naturally group together.
        
    Follow it consistently: However you choose to organize your project folders and files, it is crucial to follow your scheme consistently. It is too easy to forget to go back and fix instances where the scheme wasn't followed, likely leading to confusion. If the plan needs to be changed, change the written plan and reorganize the project files systematically.
    
    Avoid encoding information in the directory structure: You should be able to understand the contents of a file independently of the directory structure. In other words, they should be described sufficiently by their file name alone. See the next section on file naming conventions.
:::

## Example File Structure

![](images/example-file-org.png)

## Data Cleaning

Messy or poorly formatted data reflects negatively on your research and causes friction for reuse

Well-planned data collection, data entry, and data processing steps can reduce the need for cleaning later, but some cleaning will frequently be needed

Thankfully, there are strategies

## Spreadsheet Best Practices

- Only one value per cell
- Only one table per sheet
- One variable to a column, one observation to a row
- Don't encode data in the formatting or spatial layout
- Descriptive column names, with units
- Be consistent with how you handle missing data (empty? NULL? NA? 0?)

Spreadsheets aren't a lab notebook

## What Excel Can Do

- Remove duplicate rows
- Find and replace text
- Change text case
- Removing extraneous whitespace
- Adjusting numbers and dates
- Merge and split columns
- Transpose rows/columns

## More Powerful Tools

OpenRefine

-   Faceting allows for manipulation of subsets of data
-   Built-in data cleaning functions & options
-   Supports regular expressions via GREL

R (Tidyverse)

-   Powerful tabular data manipulation via `dplyr` package
-   Database joins
-   Also supports regular expressions

## Documentation

Documentation is a critical part of research data management

Describing your dataset well and documenting what you've done is not only for your own benefit -- it also benefits anyone else who wants to try and reuse your data in the future

Documentation is about **context**

## README Files

A common type of documentation included with datasets deposited into repositories is a **README file**

- General information
- Content overview
- Methodological information
- Access and reuse information
- Other data-specific details

::: {.notes}

    General Information: Dataset title, contact information for principal investigators and other key personnel, time frame and location for data collection, language information, persistent identifier (if available), and identification of funding source
    
    Content overview: Explanation of directory structure or file organization, a list of files and short descriptions of each, important relationships between files, other information necessary to understand files
    
    Methodological information: Description of method for data collection or generation, documentation of data processing/transforming/cleaning steps, details on software or instrumentation necessary to correctly interpret the data, information on quality assurance procedures (if applicable)
    
    Access and reuse information: Licenses or reuse restrictions that apply to the data, links to the associated publication and other related resources or publications (including code repositories), recommended citation for the dataset
    
    Other data-specific details: A list of variables with descriptions of each, units of measurement, definitions of codes/abbreviations/symbols used in the dataset, how missing data values have been recorded/denoted

:::

## Metadata

Sometimes the terms documentation and metadata are used interchangeably

I'll use the term metadata to refer to structured description that makes use of a formally defined standard or schema

Structured metadata fundamentally serves multiple purposes:

1. To act as documentation
2. To facilitate discovery, and
3. To allow interoperability

::: {.notes}
When the term schema is used, this usually means that a standard also has a technical specification for how the metadata should be encoded (and refers to this specification), such as in a markup language like XML.

Metadata is structured and encoded the way it is to allow it to be indexed and used in searches. In other words, it is machine-readable.
:::

## Dublin Core Element Set

:::: {.columns}

::: {.column width="50%"}
- Contributor
- Coverage
- Creator
- Date
- Description
- Format
- Identifier
- Language
:::

::: {.column width="50%"}
- Publisher
- Relation
- Rights
- Source
- Subject
- Title
- Type
:::

::::

## Some metadata fields to fill out

- Dataset Description 	
- Subject
- Keyword
- Related Work/Article 
- Notes
- License/Data Use Agreement 

::: {.aside}
[Example LibraData dataset page](https://dataverse.lib.virginia.edu/dataset.xhtml?persistentId=doi:10.18130/V3/I5UJMM) 
:::

## Data Repositories

**Research data repositories** are storage locations and services designed and intended for dataset hosting and preservation. Data repositories generally:

-   offer support for data sharing and long-term preservation
-   enable access controls for datasets
-   attach terms of use/licenses
-   provide persistent and citable identifiers
-   create landing pages for deposited datasets that display descriptive metadata

## 

![](images/datarepocrop.png)

## FAIR Data {.smaller}

-   **Findable**: A dataset is described with rich metadata, is assigned a unique and persistent identifier, and is indexed and searchable online. The metadata should include the identifier of the dataset it describes.
-   **Accessible**: The dataset and associated metadata are retrievable via their identifier using a standardized, open communications protocol that allows for authentication/authorization. Metadata should be accessible even if the data itself is no longer available.
-   **Interoperable**: Data and metadata use formally specified, standardized, and broadly applicable data/metadata schemas and vocabularies.
-   **Reusable**: Data and metadata are sufficiently described with fitting and accurate attributes, have a clear associated license covering reuse permissions, and meet the standards of the relevant domain or community.

## "Desireable Characteristics" {.smaller}

-   Free and open access
-   Clear (re)use guidance
-   Stated retention policies
-   Demonstrated capabilities for long-term planning and risk management
-   Provides curation and quality assurance
-   Ensures that every dataset has a persistent identifier and metadata
-   Data provenance tracking
-   Established data security and authentication practices

## Using Re3data

Three main approaches:

1.  Search
2.  Browse by subject
3.  Browse by content type

## Thanks!

There is a new research data management LibGuide coming soon that covers these topics

Check out the RDS newsletter and sign up for it here:

<https://library.virginia.edu/data/newsletters>

Request a consult with me at [lenard\@virginia.edu](mailto:lenard@virginia.edu)

or with the RDM team at [dmconsult\@virginia.edu](mailto:dmconsult@virginia.edu)!
